#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""attention model performance

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgURLu-n4R0ipX6svtudW0XS90ZaDapZ
"""

# Load the required libraries
import pandas as pd
# import numpy as np
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.decomposition import LatentDirichletAllocation

import torch
from transformers import BertTokenizer, BertForSequenceClassification
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# from transformers import Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

import logging
LOG_PATH = (f"./logs/att_performance.log")
logging.basicConfig(filename=LOG_PATH, filemode='w', level=logging.DEBUG, format='%(levelname)s - %(message)s')

df = pd.read_csv('./data/news_data_with_sentiment.csv')

# df=df[1:20000]

device = 'cuda' # changable
if device == 'cuda' and torch.cuda.is_available():
    device = torch.device('cuda')
elif device == 'mps' and torch.backends.mps.is_available(): # type: ignore
    device = torch.device('mps')
else:
    device = torch.device('cpu')
logging.debug(f"Computing device: {device}")

# Load the pre-trained tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2, output_hidden_states=True, output_attentions=True)
model.to(device)

# Tokenize the input text and convert it to PyTorch tensors
input_ids = []
attention_masks = []
for text in df['Item_Title']:
    encoded_dict = tokenizer.encode_plus(text,
                                          add_special_tokens=True,
                                          max_length=64,
                                          pad_to_max_length=True,
                                          return_attention_mask=True,
                                          return_tensors='pt')
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(df['top1p_views'].values)

train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)
train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)

# Define the training parameters
batch_size = 32
epochs = 1
learning_rate = 2e-5

# Define the optimizer and loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

# Train the model
logging.debug("Enter Traning")
for epoch in range(epochs):
    model.train()
    train_loss = 0
    for i in range(0, len(train_inputs), batch_size):
        inputs = train_inputs[i:i+batch_size]
        masks = train_masks[i:i+batch_size]
        labels = train_labels[i:i+batch_size]

        inputs = inputs.to(device)
        masks = masks.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = criterion(outputs[1], labels)

        train_loss += loss.item()

        loss.backward()
        optimizer.step()
        # logging.debug(i)
    logging.debug(epoch)

    # Evaluate the model on the validation set
    model.eval()
    eval_loss = 0
    num_correct = 0
    predictions = []
    true_labels = []
    attention_weights = []
    with torch.no_grad():
        for i in range(0, len(validation_inputs), batch_size):
            inputs = validation_inputs[i:i+batch_size]
            masks = validation_masks[i:i+batch_size]
            labels = validation_labels[i:i+batch_size]

            inputs = inputs.to(device)
            masks = masks.to(device)
            labels = labels.to(device)


            outputs = model(inputs, attention_mask=masks, labels=labels)
            loss = criterion(outputs[1], labels)

            eval_loss += loss.item()

            _, preds = torch.max(outputs[1], dim=1)
            num_correct += torch.sum(preds == labels)
            predictions.extend(preds.cpu().numpy().tolist())
            true_labels.extend(labels.cpu().numpy().tolist())
            
            # Get attention weights
            attention_weight = []
            for j in range(len(inputs)):
                input_ids = inputs[j]
                attention_mask = masks[j]
                output = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))
                attn_weights = torch.softmax(output.attentions[-1][0], dim=-1)
                attn_weights = attn_weights.squeeze()
                attention_weight.append(attn_weights.cpu().numpy().tolist())
            attention_weights.extend(attention_weight)
            

    # Calculate the accuracy and logging.debug the results
    accuracy = num_correct / len(validation_labels)
    logging.debug(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_labels):.4f}, Validation Loss: {eval_loss / len(validation_labels):.4f}, Validation Accuracy: {accuracy:.4f}')

    # logging.debug classification report and confusion matrix
    logging.debug(classification_report(true_labels, predictions))
    confusion_matrix = pd.crosstab(pd.Series(true_labels), pd.Series(predictions), rownames=['True'], colnames=['Predicted'])
    logging.debug(confusion_matrix)
    confusion_matrix.to_csv('./att_results/confusion matrix.csv', index=False)
    # Save attention weights to a CSV file
    attention_df = pd.DataFrame({'text': validation_inputs.cpu().numpy().tolist(),
                                'label': true_labels,
                                'prediction': predictions,
                                'attention_weights': attention_weights})
    attention_df.to_csv('./att_results/attention_weights.csv', index=False)

confusion_matrix.to_csv('./att_results/confusion matrix.csv', index=False)

# # Train the model
# for epoch in range(epochs):
#     model.train()
#     train_loss = 0
#     for i in range(0, len(train_inputs), batch_size):
#         inputs = train_inputs[i:i+batch_size]
#         masks = train_masks[i:i+batch_size]
#         labels = train_labels[i:i+batch_size]

#         optimizer.zero_grad()

#         outputs = model(inputs, attention_mask=masks, labels=labels)
#         loss = criterion(outputs[1], labels)

#         train_loss += loss.item()

#         loss.backward()
#         optimizer.step()
#         logging.debug(i)
#     logging.debug(epoch)

#     # Evaluate the model on the validation set
#     model.eval()
#     eval_loss = 0
#     num_correct = 0
#     with torch.no_grad():
#         for i in range(0, len(validation_inputs), batch_size):
#             inputs = validation_inputs[i:i+batch_size]
#             masks = validation_masks[i:i+batch_size]
#             labels = validation_labels[i:i+batch_size]

#             outputs = model(inputs, attention_mask=masks, labels=labels)
#             loss = criterion(outputs[1], labels)

#             eval_loss += loss.item()

#             _, predictions = torch.max(outputs[1], dim=1)
#             num_correct += torch.sum(predictions == labels)
#             logging.debug(i)
#     # Calculate the accuracy and logging.debug the results
#     accuracy = num_correct / len(validation_labels)
#     logging.debug(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_labels):.4f}, Validation Loss: {eval_loss / len(validation_labels):.4f}, Validation Accuracy: {accuracy:.4f}')


# tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
# model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", output_hidden_states=True, output_attentions=True)

# input_text = "This is a positive sentence"
# input_ids = torch.tensor(tokenizer.encode(input_text, add_special_tokens=True)).unsqueeze(0)  # Batch size 1
# attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)

# outputs = model(input_ids, attention_mask=attention_mask)
# attn_weights = torch.softmax(outputs.attentions[-1][0], dim=-1)
# attn_weights = attn_weights.squeeze()
# attention_words = tokenizer.convert_ids_to_tokens(input_ids[0])[1:-1]  # remove special tokens

# for i, word in enumerate(attention_words):
#     logging.debug(f"{word}: {attn_weights[i]}")

# # Train the model
# for epoch in range(epochs):
#     model.train()
#     train_loss = 0
#     for i in range(0, len(train_inputs), batch_size):
#         inputs = train_inputs[i:i+batch_size]
#         masks = train_masks[i:i+batch_size]
#         labels = train_labels[i:i+batch_size]

#         optimizer.zero_grad()

#         outputs = model(inputs, attention_mask=masks, labels=labels)
#         loss = criterion(outputs[1], labels)

#         train_loss += loss.item()

#         loss.backward()
#         optimizer.step()
#         logging.debug(i)
#     logging.debug(epoch)

#     # Evaluate the model on the validation set
#     model.eval()
#     eval_loss = 0
#     num_correct = 0
#     predictions = []
#     true_labels = []
#     attention_weights = []
#     with torch.no_grad():
#         for i in range(0, len(validation_inputs), batch_size):
#             inputs = validation_inputs[i:i+batch_size]
#             masks = validation_masks[i:i+batch_size]
#             labels = validation_labels[i:i+batch_size]

#             outputs = model(inputs, attention_mask=masks, labels=labels)
#             loss = criterion(outputs[1], labels)

#             eval_loss += loss.item()

#             _, preds = torch.max(outputs[1], dim=1)
#             num_correct += torch.sum(preds == labels)
#             predictions.extend(preds.cpu().numpy().tolist())
#             true_labels.extend(labels.cpu().numpy().tolist())
            
#             # Get attention weights
#             attention_weight = []
#             for j in range(len(inputs)):
#                 input_ids = inputs[j]
#                 attention_mask = masks[j]
#                 output = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))
#                 logging.debug(output)
#                 assert output is not None
#                 attn_weights = torch.softmax(output.attentions[-1][0], dim=-1)
#                 attn_weights = attn_weights.squeeze()
#                 attention_weight.append(attn_weights.cpu().numpy().tolist())
#             attention_weights.extend(attention_weight)
            

#     # Calculate the accuracy and logging.debug the results
#     accuracy = num_correct / len(validation_labels)
#     logging.debug(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_labels):.4f}, Validation Loss: {eval_loss / len(validation_labels):.4f}, Validation Accuracy: {accuracy:.4f}')

#     # logging.debug classification report and confusion matrix
#     logging.debug(classification_report(true_labels, predictions))
#     confusion_matrix = pd.crosstab(pd.Series(true_labels), pd.Series(predictions), rownames=['True'], colnames=['Predicted'])
#     logging.debug(confusion_matrix)

#     # Save attention weights to a CSV file
#     attention_df = pd.DataFrame({'text': validation_inputs.cpu().numpy().tolist(),
#                                 'label': true_labels,
#                                 'prediction': predictions,
#                                 'attention_weights': attention_weights})
#     attention_df.to_csv('attention_weights.csv', index=False)

# #Calculate the accuracy and logging.debug the results
# accuracy = num_correct / len(validation_labels)
# logging.debug(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_labels):.4f}, Validation Loss: {eval_loss / len(validation_labels):.4f}, Validation Accuracy: {accuracy:.4f}')

# # logging.debug classification report and confusion matrix
# logging.debug(classification_report(true_labels, predictions))
# confusion_matrix = pd.crosstab(pd.Series(true_labels), pd.Series(predictions), rownames=['True'], colnames=['Predicted'])
# logging.debug(confusion_matrix)



# # Load your dataset (replace this with your actual dataframe)
# # df = pd.read_csv("your_dataset.csv")

# # Load the pre-trained BERT model and tokenizer
# model_name = 'bert-base-chinese'
# tokenizer = BertTokenizer.from_pretrained(model_name)
# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# # Tokenize and prepare the input dataset
# input_texts = df['Item_Title'].tolist()
# attention_masks = []
# input_ids = []

# for text in input_texts:
#     encoded_dict = tokenizer.encode_plus(
#         text,
#         add_special_tokens=True,
#         max_length=128,
#         pad_to_max_length=True,
#         return_attention_mask=True,
#         return_tensors='pt',
#     )
#     input_ids.append(encoded_dict['input_ids'])
#     attention_masks.append(encoded_dict['attention_mask'])

# input_ids = torch.cat(input_ids, dim=0)
# attention_masks = torch.cat(attention_masks, dim=0)
# labels = torch.tensor(df['top1p_views'].values)

# # Split the data into training and validation sets
# train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state=42, stratify=labels)
# train_masks, val_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, stratify=labels)

# # Create DataLoader for training and validation sets
# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# batch_size = 20
# train_data = TensorDataset(train_inputs, train_masks, train_labels)
# train_sampler = RandomSampler(train_data)
# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# val_data = TensorDataset(val_inputs, val_masks, labels)
# val_sampler = SequentialSampler(val_data)
# val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# # Train the model
# training_args = TrainingArguments(
#     output_dir='./results',
#     num_train_epochs=3,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     warmup_steps=500,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     evaluation_strategy="epoch",
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_data,
#     eval_dataset=val_data,
# )

# trainer.train()

# # Extract the attention weights
# model.eval()
# for batch in val_dataloader:
#     input_ids, attention_mask, labels = batch

#     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
#     attention_weights = outputs[-1]

#     # logging.debug the most important words for the first input in the batch
#     tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
#     attention_scores = attention_weights[0][0][:len(tokens)]
#     sorted_indices = attention_scores.argsort(descending=True)

#     logging.debug('Input:', input_texts[0])
#     logging.debug('Label:', labels[0])
#     logging.debug('Predicted:', torch.argmax(outputs[1][0]))
#     logging.debug('Top 5 most important words:')
#     for i in sorted_indices[:5]:
#         logging.debug(tokens[i], attention_scores[i])

# import pandas as pd
# import torch
# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
# from sklearn.model_selection import train_test_split

# # Load your dataset (replace this with your actual dataframe)
# # df = pd.read_csv("your_dataset.csv")

# # Load the pre-trained BERT model and tokenizer
# model_name = 'bert-base-chinese'
# tokenizer = BertTokenizer.from_pretrained(model_name)
# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# # Tokenize and prepare the input dataset
# input_texts = df['Item_Title'].tolist()
# attention_masks = []
# input_ids = []

# for text in input_texts:
#     encoded_dict = tokenizer.encode_plus(
#         text,
#         add_special_tokens=True,
#         max_length=128,
#         pad_to_max_length=True,
#         return_attention_mask=True,
#         return_tensors='pt',
#     )
#     input_ids.append(encoded_dict['input_ids'])
#     attention_masks.append(encoded_dict['attention_mask'])

# input_ids = torch.cat(input_ids, dim=0)
# attention_masks = torch.cat(attention_masks, dim=0)
# labels = torch.tensor(df['top1p_views'].values)

# # Split the data into training and validation sets
# train_indices, val_indices = train_test_split(range(len(df)), random_state=42, stratify=labels)
# train_inputs, train_masks, train_labels = input_ids[train_indices], attention_masks[train_indices], labels[train_indices]
# val_inputs, val_masks, val_labels = input_ids[val_indices], attention_masks[val_indices], labels[val_indices]

# # Create DataLoader for training and validation sets
# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# batch_size = 20
# train_data = TensorDataset(train_inputs, train_masks, train_labels)
# train_sampler = RandomSampler(train_data)
# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# val_data = TensorDataset(val_inputs, val_masks, val_labels)
# val_sampler = SequentialSampler(val_data)
# val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# # Train the model
# training_args = TrainingArguments(
#     output_dir='./results',
#     num_train_epochs=3,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     warmup_steps=500,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     evaluation_strategy="epoch",
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_data,
#     eval_dataset=val_data,
# )

# trainer.train()

# # Extract the attention weights
# model.eval()
# for batch in val_dataloader:
#     input_ids, attention_mask, labels = batch

#     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
#     attention_weights = outputs[-1]



# df_1 = df[df['top1p_views'] == 1]
# titles = df_1['Item_Title'].tolist()

# # Tokenize the titles and convert to input IDs and attention masks
# inputs = tokenizer(titles, padding=True, truncation=True, return_tensors='pt')
# input_ids = inputs['input_ids']
# attention_masks = inputs['attention_mask']

# # Evaluate the model on the titles to obtain the attention weights
# model.eval()
# with torch.no_grad():
#     outputs = model(input_ids, attention_mask=attention_masks, output_attentions=True)
#     attention_weights = outputs.attentions[-1]

# # Find the most important word in each title
# max_weights, indices = torch.max(attention_weights, dim=2)
# important_words = []
# for i in range(len(titles)):
#     important_word = tokenizer.decode(input_ids[i][indices[i]])
#     important_words.append(important_word)

# # Create a new DataFrame with the titles and important words
# df_important_words = pd.DataFrame({'item_title': titles, 'important_word': important_words})

# # Save the results to a CSV file
# df_important_words.to_csv('/content/drive/MyDrive/virality analysis 2.0/important_words_1.csv', index=False)

# import pandas as pd
# import torch
# from sklearn.model_selection import train_test_split
# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# # Load your dataset (replace this with your actual dataframe)
# # df = pd.read_csv("your_dataset.csv")

# # Load the pre-trained BERT model and tokenizer
# model_name = 'bert-base-chinese'
# tokenizer = BertTokenizer.from_pretrained(model_name)
# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# # Tokenize and prepare the input dataset
# input_texts = df['Item_Title'].tolist()
# attention_masks = []
# input_ids = []

# for text in input_texts:
#     encoded_dict = tokenizer.encode_plus(
#         text,
#         add_special_tokens=True,
#         max_length=128,
#         pad_to_max_length=True,
#         return_attention_mask=True,
#         return_tensors='pt',
#     )
#     input_ids.append(encoded_dict['input_ids'])
#     attention_masks.append(encoded_dict['attention_mask'])

# input_ids = torch.cat(input_ids, dim=0)
# attention_masks = torch.cat(attention_masks, dim=0)
# labels = torch.tensor(df['top1p_views'].values)

# # Split the data into training and validation sets
# train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state=42, stratify=labels)
# train_masks, val_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, stratify=labels)

# # Create DataLoader for training and validation sets
# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# batch_size = 20
# train_data = TensorDataset(train_inputs, train_masks, train_labels)
# train_sampler = RandomSampler(train_data)
# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# val_data = TensorDataset(val_inputs, val_masks, val_labels)
# val_sampler = SequentialSampler(val_data)
# val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# # Train the model
# training_args = TrainingArguments(
#     output_dir='./results',
#     num_train_epochs=3,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     warmup_steps=500,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     evaluation_strategy="epoch",
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_data,
#     eval_dataset=val_data,
# )

# trainer.train()

# eval_results = trainer.evaluate()

# # logging.debug the evaluation metrics
# logging.debug("Evaluation results:", eval_results)

# import pytorch_lightning as pl
# from torch.utils.data import Dataset

# class TextDataset(Dataset):
#     def __init__(self, input_ids, attention_masks, labels):
#         self.input_ids = input_ids
#         self.attention_masks = attention_masks
#         self.labels = labels
        
#     def __len__(self):
#         return len(self.labels)
    
#     def __getitem__(self, idx):
#         return {
#             'input_ids': self.input_ids[idx],
#             'attention_mask': self.attention_masks[idx],
#             'label': self.labels[idx]
#         }

# class TextDataModule(pl.LightningDataModule):
#     def __init__(self, train_data, val_data, batch_size=20):
#         super().__init__()
#         self.batch_size = batch_size
#         self.train_data = train_data
#         self.val_data = val_data

#     def train_dataloader(self):
#         return DataLoader(self.train_data, sampler=RandomSampler(self.train_data), batch_size=self.batch_size)

#     def val_dataloader(self):
#         return DataLoader(self.val_data, sampler=SequentialSampler(self.val_data), batch_size=self.batch_size)

# class TextClassifier(pl.LightningModule):
#     def __init__(self, model):
#         super().__init__()
#         self.model = model

#     def forward(self, input_ids, attention_mask):
#         return self.model(input_ids, attention_mask=attention_mask)

#     def training_step(self, batch, batch_idx):
#         input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['label']
#         outputs = self(input_ids, attention_mask)
#         loss = outputs.loss
#         self.log("train_loss", loss, prog_bar=True)
#         return loss

#     def validation_step(self, batch, batch_idx):
#         input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['label']
#         outputs = self(input_ids, attention_mask)
#         val_loss = outputs.loss
#         self.log("val_loss", val_loss, prog_bar=True)

#     def configure_optimizers(self):
#         optimizer = torch.optim.AdamW(self.parameters(), lr=2e-5)
#         return optimizer

# train_dataset = TextDataset(train_inputs, train_masks, train_labels)
# val_dataset = TextDataset(val_inputs, val_masks, val_labels)

# data_module = TextDataModule(train_dataset, val_dataset)
# text_classifier = TextClassifier(model)

# #trainer = pl.Trainer(max_epochs=3, gpus=1, progress_bar_refresh_rate=20)

# trainer = pl.Trainer(max_epochs=3)
# trainer.fit(text_classifier, data_module)

# def test_step(self, batch, batch_idx):
#     input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['label']
#     outputs = self(input_ids, attention_mask)
#     test_loss = outputs.loss
#     self.log("test_loss", test_loss, prog_bar=True)

# # Add the test_step method to the TextClassifier class
# TextClassifier.test_step = test_step

# trainer.test(datamodule=data_module)

# """1pip install pytorch-lightning"""

# import torch
# import pandas as pd
# import numpy as np
# from sklearn.model_selection import train_test_split
# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

# # Load the pre-trained BERT model and tokenizer
# model_name = 'bert-base-chinese'
# tokenizer = BertTokenizer.from_pretrained(model_name)
# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# # Tokenize and prepare the input dataset
# input_texts = df['Item_Title'].tolist()
# attention_masks = []
# input_ids = []

# for text in input_texts:
#     encoded_dict = tokenizer.encode_plus(
#         text,
#         add_special_tokens=True,
#         max_length=128,
#         pad_to_max_length=True,
#         return_attention_mask=True,
#         return_tensors='pt',
#     )
#     input_ids.append(encoded_dict['input_ids'])
#     attention_masks.append(encoded_dict['attention_mask'])

# input_ids = torch.cat(input_ids, dim=0)
# attention_masks = torch.cat(attention_masks, dim=0)
# labels = torch.tensor(df['top1p_views'].values)

# # Split the data into training and validation sets
# train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state=42, stratify=labels)
# train_masks, val_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, stratify=labels)

# # Create DataLoader for training and validation sets
# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

# batch_size = 20
# train_data = TensorDataset(train_inputs, train_masks, train_labels)
# train_sampler = RandomSampler(train_data)
# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

# val_data = TensorDataset(val_inputs, val_masks, val_labels)
# val_sampler = SequentialSampler(val_data)
# val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# # Train the model
# training_args = TrainingArguments(
#     output_dir='./results',
#     num_train_epochs=3,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     warmup_steps=500,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     load_best_model_at_end=True,
#     metric_for_best_model="f1",
#     evaluation_strategy="epoch",
# )

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = np.argmax(logits, axis=-1)
#     precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
#     acc = accuracy_score(labels, predictions)
#     return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_data,
#     eval_dataset=val_data,
#     compute_metrics=compute_metrics,
# )

# trainer.train()

# # logging.debug performance matrix
# eval_metrics = trainer.evaluate()
# logging.debug(eval_metrics)