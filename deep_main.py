#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""attention model performance

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WgURLu-n4R0ipX6svtudW0XS90ZaDapZ
"""

# Load the required libraries
import os
import pandas as pd
from tqdm import tqdm
import torch
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from models import Deep

import logging
LOG_PATH = (f"./logs/deep.log")
logging.basicConfig(filename=LOG_PATH, filemode='w', level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
DATA_PATH = './data/feed_data.pt'


df = pd.read_csv('./data/news_data_with_y1030_sentiment.csv')

# config variables
mode = 'train'
percent = 5 

device = 'cuda' # changable
if device == 'cuda' and torch.cuda.is_available():
    device = torch.device('cuda')
elif device == 'mps' and torch.backends.mps.is_available(): # type: ignore
    device = torch.device('mps')
else:
    device = torch.device('cpu')
logging.debug(f"Computing device: {device}")

# parse inputs
if os.path.isfile(DATA_PATH):
    inputs, labels = torch.load(DATA_PATH)
else:
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    df['Item_Author_f'], author_index = pd.factorize(df['Item_Author'])
    df['Company_ID_f'], company_index = pd.factorize(df['Company_ID'])
    df['sentiment_f'], sentiment_index = pd.factorize(df['sentiment'])

    inputs = []
    for index, row in tqdm(df.iterrows(), total=len(df)):
        row_input = []
        # news title [0,1]
        title = row['Item_Title']
        encoded_dict = tokenizer.encode_plus(title,
                                            add_special_tokens=True,
                                            max_length=64,
                                            pad_to_max_length=True,
                                            return_attention_mask=True,
                                            return_tensors='pt')
        row_input.append(encoded_dict['input_ids'])
        row_input.append(encoded_dict['attention_mask'])
        # news text [2,3]
        text = row['news_text']
        encoded_dict = tokenizer.encode_plus(text,
                                            add_special_tokens=True,
                                            max_length=128,
                                            pad_to_max_length=True,
                                            return_attention_mask=True,
                                            return_tensors='pt')
        row_input.append(encoded_dict['input_ids'])
        row_input.append(encoded_dict['attention_mask'])
        # author index [4]
        author = row['Item_Author_f']
        row_input.append(torch.tensor(author))
        # company index [5]
        company = row['Company_ID_f']
        row_input.append(torch.tensor(company))
        # sentiment index [6]
        sentiment = row['sentiment_f']
        row_input.append(torch.tensor(sentiment))

    inputs = torch.cat(inputs, dim=0)

    # label
    labels = torch.tensor(df[f'top{percent}p_views'].values)

    #save procussed data
    torch.save((inputs, labels), DATA_PATH)

# split data
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(inputs, labels, random_state=42, test_size=0.1)

# setup model
model = Deep(len(author_index), len(company_index), len(sentiment_index))
criterion = torch.nn.CrossEntropyLoss()

# Define the training parameters
batch_size = 16
epochs = 5
learning_rate = 1e-6

# Train the model
if mode == 'train':

    # Load the pre-trained tokenizer and model
    model.to(device)

    # Define the optimizer and loss function
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    logging.debug("Enter Traning")
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for i in range(0, len(train_inputs), batch_size):
            inputs = train_inputs[i:i+batch_size]
            labels = train_labels[i:i+batch_size]

            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()

            title_embed = model(inputs[0], attention_mask=inputs[1])
            text_embed = model(inputs[2], attention_mask=inputs[3])

            outputs = model(inputs)
            output_logit = outputs[1].softmax(dim=1)
            print(output_logit, labels)
            loss = criterion(output_logit, labels)
            print(loss)

            train_loss += loss.item()

            loss.backward()
            optimizer.step()
            if i%10000==0:
                logging.debug(f"train:{i}")
        logging.debug(f"train epoch:{epoch}, loss:{train_loss}")

        torch.save(model.state_dict(),f"./models/attention_model_{percent}_{epoch}_sig.pt")

elif mode == 'test':
    logging.debug("enter testing mode")
    for epoch in range(epochs):
        logging.debug(f"{epoch}")
        param = torch.load(f"./models/attention_model_{percent}_{epoch}.pt")
        model.load_state_dict(param)
        model.to(device)
        logging.debug(f"data size: {len(validation_inputs)}")

        # Evaluate the model on the validation set
        model.eval()
        batch_size = 1
        eval_loss = 0
        num_correct = 0
        predictions = []
        true_labels = []
        attention_weights = []
        with torch.no_grad():
            for i in range(0, len(validation_inputs),batch_size):
                inputs = validation_inputs[i:i+batch_size]
                labels = validation_labels[i:i+batch_size]

                inputs = inputs.to(device)
                labels = labels.to(device)


                outputs = model(inputs)
                loss = criterion(outputs[1], labels)

                attn_weights = torch.softmax(outputs.attentions[-1][0], dim=-1).squeeze()
                attn_value = torch.unsqueeze(attn_weights, 0)

                eval_loss += loss.item()

                _, preds = torch.max(outputs[1], dim=1)
                # print(outputs[1])
                # print(labels)
                num_correct += torch.sum(preds == labels)
                predictions.extend(preds.cpu().numpy().tolist())
                true_labels.extend(labels.cpu().numpy().tolist())
                
                # Get attention weights
                # attention_weight = []
                # for j in range(len(inputs)):
                #     input_ids = inputs[j]
                #     attention_mask = masks[j]
                #     output = model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))
                #     attn_weights = torch.softmax(output.attentions[-1][0], dim=-1)
                #     attn_weights = attn_weights.squeeze()
                #     attention_weight.append(attn_weights.cpu().numpy().tolist())
                # attention_weights.extend(attention_weight)
                # print(validation_inputs.cpu().numpy().tolist())
                # print(inputs.shape, labels.shape, preds.shape, attn_value.shape) 
                attention_df = pd.Series({'text': inputs.cpu().numpy(),
                                    'label': labels.cpu().numpy(),
                                    'prediction': preds.cpu().numpy(),
                                    'attention_weights': attn_value.cpu().numpy()})
                attention_df.to_frame().T.to_csv(f'./att_results/attention_weights_{percent}_{epoch}.csv', mode='a', index=False, header=False)
                # del attn_weights
                # del attention_df
                # break
                if i%10000==0:
                    logging.debug(f"test:{i}")
                

        # Calculate the accuracy and logging.debug the results
        accuracy = num_correct / len(validation_labels)
        logging.debug(f'Validation Loss: {eval_loss / len(validation_labels):.4f}, Validation Accuracy: {accuracy:.4f}')

        # logging.debug classification report and confusion matrix
        logging.debug(f"\n{classification_report(true_labels, predictions)}")
        confusion_matrix = pd.crosstab(pd.Series(true_labels), pd.Series(predictions), rownames=['True'], colnames=['Predicted'])
        logging.debug(f"\n{confusion_matrix}")
        confusion_matrix.to_csv(f'./att_results/confusion matrix_{percent}_{epoch}.csv', index=False)
        # Save attention weights to a CSV file
        # attention_df = pd.DataFrame({'text': validation_inputs.cpu().numpy().tolist(),
        #                             'label': true_labels,
        #                             'prediction': predictions,
        #                             'attention_weights': attention_weights})
        # attention_df.to_csv('./att_results/attention_weights.csv', mode='a', index=False, header=False)